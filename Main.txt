
import numpy as np
import torch
from operator import truediv
import torch.utils.data as Data
from sklearn import metrics, preprocessing
from sklearn.preprocessing import MinMaxScaler
from sklearn.decomposition import PCA
from sklearn.model_selection import train_test_split
from sklearn.metrics import confusion_matrix, accuracy_score, classification_report, cohen_kappa_score
from plotly.offline import init_notebook_mode
import matplotlib.pyplot as plt
import scipy.io as sio
import os
!pip install spectral
import spectral
import cv2
from IPython import display
import math
import torch.nn as nn
import torch.nn.functional as F
from torch.nn import init
from torchsummary import summary
!pip install einops
from einops import rearrange, repeat
init_notebook_mode(connected=True)
%matplotlib inline

!pip install torch-optimizer
import torch_optimizer as optim2
from torch import optim
import time
import collections
import imgaug.augmenters as iaa
import threading
import sys
import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, TensorDataset
from sklearn.model_selection import train_test_split
from sklearn.metrics import r2_score, mean_squared_error
import matplotlib.pyplot as plt
import random



# Define function to calculate RÂ² score
def calculate_r2(targets, predictions):
    return r2_score(targets, predictions)

# Define function to calculate RMSE
def calculate_rmse(targets, predictions):
    return np.sqrt(mean_squared_error(targets, predictions))

# Define function to calculate RPD
def calculate_rpd(targets, predictions):
    return np.std(targets) / calculate_rmse(targets, predictions)

#Data Loading

import os
import h5py
import pandas as pd
import re

# Paths to the folders containing the HDF5 and CSV files
hdf5_folder_path = 'Documents/Blueberry_Dataset/'  # Update this path
csv_file_path = 'Documents/Blueberry_Dataset/Sheet_SSC/csv_blueberry_single_row.csv'  # Update this path

# List to store data from HDF5 files and corresponding CSV data
hdf5_data = []
csv_data = []

# Function to sort filenames based on the numerical values in them
def sort_filenames(filenames):
    def extract_numbers(filename):
        numbers = re.findall(r'\d+', filename)
        return list(map(int, numbers))

    return sorted(filenames, key=extract_numbers)

# Function to read data from HDF5 files
def read_hdf5_files(folder_path):
    filenames = [f for f in os.listdir(folder_path) if f.endswith('.h5')]
    sorted_filenames = sort_filenames(filenames)

    for file_name in sorted_filenames:
        file_path = os.path.join(folder_path, file_name)
        with h5py.File(file_path, 'r') as f:
            # Iterate through the keys in the HDF5 file to find the correct dataset name
            for key in f.keys():
                data = f[key][()]  # Try reading the dataset using the identified key
                hdf5_data.append((file_name, data))

# Function to read data from CSV file
def read_csv_file(file_path):
    csv_df = pd.read_csv(file_path)
    csv_values = csv_df.iloc[:, 1]  # Extract values from the second column
    csv_data.extend(csv_values.tolist())

# Call functions to read data
read_hdf5_files(hdf5_folder_path)
read_csv_file(csv_file_path)

# Verify the lengths of both datasets
print("Number of HDF5 files read:", len(hdf5_data))
print("Number of values read from CSV file:", len(csv_data))

# Labeling the HDF5 data with CSV values
labeled_data = []
for i, (file_name, data) in enumerate(hdf5_data):
    if i < len(csv_data):  # Ensure there are enough labels for the HDF5 data
        labeled_data.append((file_name, data, csv_data[i]))

# Print the filenames, shapes, and their corresponding labels
for item in labeled_data:
    print("Filename:", item[0])
    print("Shape:", item[1].shape)
    print("Label:", item[2])

# Optional: Save labeled data to a new file or process as needed

# Preprocessing 

import os
import re
import h5py
import numpy as np
import pandas as pd
from sklearn.decomposition import PCA

def load_dataset(folder_path, csv_file_path, num_components=15):
    # Load the CSV file containing the corresponding values
    csv_data = pd.read_csv(csv_file_path)

    # Extract the second column of the CSV file (assuming index 1 is the second column)
    csv_values = csv_data.iloc[:, 1].values

    # Initialize list to store HDF5 data and corresponding filenames
    hdf5_data = []
    hdf5_filenames = []

    # Define a function to extract numerical parts from filenames for proper sorting
    def natural_key(filename):
        return [int(text) if text.isdigit() else text for text in re.split(r'(\d+)', filename)]

    # Get the list of HDF5 files and sort them using the natural key
    hdf5_files = sorted([file for file in os.listdir(folder_path) if file.endswith('.h5')], key=natural_key)

    # Loop through the sorted HDF5 files
    for file_name in hdf5_files:
        file_path = os.path.join(folder_path, file_name)
        with h5py.File(file_path, 'r') as f:
            # Iterate through keys to find the correct dataset name
            for key in f.keys():
                data = f[key][()]  # Try reading with the found key
                hdf5_data.append(data)
                hdf5_filenames.append(file_name)

    # Stack the individual NumPy arrays into a single array
    if len(hdf5_data) > 0:
        hdf5_data = np.stack(hdf5_data, axis=0)  # Stack the arrays
    else:
        raise ValueError("No HDF5 data found in the specified folder.")

    # Reshape the data for PCA: (num_samples, height, width, num_bands) to (num_samples*height*width, num_bands)
    num_samples, height, width, num_bands = hdf5_data.shape
    reshaped_data = hdf5_data.reshape(-1, num_bands)

    # Apply PCA
    pca = PCA(n_components=num_components)
    pca_data = pca.fit_transform(reshaped_data)

    # Reshape back to original shape but with reduced bands
    pca_data = pca_data.reshape(num_samples, height, width, num_components)

    # Print the significant bands
    components = pca.components_
    significant_bands = np.argsort(-np.abs(components), axis=1)[:, :num_components]
    
    for i, component in enumerate(significant_bands):
        print(f"Principal Component {i + 1}: Significant Bands: {component}")

    # Check the number of samples loaded from HDF5 files
    print("Number of samples loaded from HDF5 files:", len(pca_data))

    # Check the number of samples loaded from the CSV file
    print("Number of samples loaded from CSV file:", len(csv_values))

    # Check data consistency
    num_samples_hdf5 = pca_data.shape[0]
    num_samples_csv = len(csv_values)
    if num_samples_hdf5 != num_samples_csv:
        raise ValueError("Mismatched number of samples between HDF5 files and CSV data.")

    return pca_data, csv_values, hdf5_filenames, csv_data

num_components = 15  # Number of principal components to keep

hdf5_data, csv_values, hdf5_filenames, csv_data = load_dataset(hdf5_folder_path, csv_file_path, num_components)

# Verify the loaded data
print("Shape of HDF5 data array:", hdf5_data.shape)
print("Number of samples loaded from HDF5 files:", len(hdf5_data))
print("Number of samples loaded from CSV file:", len(csv_values))


#Data Augmentation

def augment_and_replicate_data(pca_data, csv_values, pca_filenames, augmentation, n_augmentations=1):
    augmented_pca_data = []
    augmented_csv_values = []
    augmented_filenames = []

    for idx in range(len(pca_data)):
        original_data = pca_data[idx]
        original_csv_value = csv_values[idx]
        original_filename = pca_filenames[idx]

        # Append original data and corresponding CSV value
        augmented_pca_data.append(original_data)
        augmented_csv_values.append(original_csv_value)
        augmented_filenames.append(f"{original_filename}_original")

        # Apply augmentation n_augmentations times to the original data
        for aug_idx in range(n_augmentations):
            augmented_data = augmentation.augment_image(original_data)
            augmented_pca_data.append(augmented_data)
            augmented_csv_values.append(original_csv_value)
            augmented_filenames.append(f"{original_filename}_aug{aug_idx+1}")

    # Convert lists to NumPy arrays
    augmented_pca_data = np.array(augmented_pca_data)
    augmented_csv_values = np.array(augmented_csv_values)

    return augmented_pca_data, augmented_csv_values, augmented_filenames


# Load dataset with PCA
pca_data, csv_values, hdf5_filenames, csv_data = load_dataset(hdf5_folder_path, csv_file_path, num_components)
train_data, test_data, train_labels, test_labels, train_filenames, test_filenames = train_test_split(pca_data, csv_values, hdf5_filenames, test_size=0.2, random_state=42)

# Define augmentation pipeline
augmentation = iaa.Sequential([
    iaa.Affine(translate_px={"x": (-10, 10), "y": (-10, 10)}),  # Translation
    iaa.Affine(rotate=(-25, 25)),  # Rotation
    iaa.Affine(scale=(0.8, 1.2)),  # Scaling
    iaa.Fliplr(0.5)  # Flipping
])

# Number of augmentations per original sample
n_augmentations = 20

# Augment and replicate data
augmented_pca_data, augmented_csv_values, augmented_filenames = augment_and_replicate_data(train_data, train_labels, train_filenames, augmentation, n_augmentations)

# Verify shapes and consistency after augmentation
print("Augmented PCA data shape:", augmented_pca_data.shape)
print("Augmented CSV data shape:", augmented_csv_values.shape)

# Verification of consistency
if augmented_pca_data.shape[0] == len(augmented_csv_values):
    print("Number of samples matches between augmented PCA data and augmented CSV data.")
else:
    print("Mismatched number of samples between augmented PCA data and augmented CSV data.")

# Print first 10 corresponding PCA data and CSV values
for i, (filename, data, csv_value) in enumerate(zip(augmented_filenames, augmented_pca_data, augmented_csv_values)):
    if i >= 10:
        break
    print(f"Filename {i+1}: {filename}")
    print(f"Augmented PCA Data {i+1}: {data.shape}")
    print(f"Augmented CSV Value {i+1}: {csv_value}")
    print("---")

