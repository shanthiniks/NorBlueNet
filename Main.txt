
import numpy as np
import torch
from operator import truediv
import torch.utils.data as Data
from sklearn import metrics, preprocessing
from sklearn.preprocessing import MinMaxScaler
from sklearn.decomposition import PCA
from sklearn.model_selection import train_test_split
from sklearn.metrics import confusion_matrix, accuracy_score, classification_report, cohen_kappa_score
from plotly.offline import init_notebook_mode
import matplotlib.pyplot as plt
import scipy.io as sio
import os
!pip install spectral
import spectral
import cv2
from IPython import display
import math
import torch.nn as nn
import torch.nn.functional as F
from torch.nn import init
from torchsummary import summary
!pip install einops
from einops import rearrange, repeat
init_notebook_mode(connected=True)
%matplotlib inline

!pip install torch-optimizer
import torch_optimizer as optim2
from torch import optim
import time
import collections
import imgaug.augmenters as iaa
import threading
import sys
import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, TensorDataset
from sklearn.model_selection import train_test_split
from sklearn.metrics import r2_score, mean_squared_error
import matplotlib.pyplot as plt
import random



# Define function to calculate RÂ² score
def calculate_r2(targets, predictions):
    return r2_score(targets, predictions)

# Define function to calculate RMSE
def calculate_rmse(targets, predictions):
    return np.sqrt(mean_squared_error(targets, predictions))

# Define function to calculate RPD
def calculate_rpd(targets, predictions):
    return np.std(targets) / calculate_rmse(targets, predictions)

#Data Loading

import os
import h5py
import pandas as pd
import re

# Paths to the folders containing the HDF5 and CSV files
hdf5_folder_path = 'Documents/Blueberry_Dataset/'  # Update this path
csv_file_path = 'Documents/Blueberry_Dataset/Sheet_SSC/csv_blueberry_single_row.csv'  # Update this path

# List to store data from HDF5 files and corresponding CSV data
hdf5_data = []
csv_data = []

# Function to sort filenames based on the numerical values in them
def sort_filenames(filenames):
    def extract_numbers(filename):
        numbers = re.findall(r'\d+', filename)
        return list(map(int, numbers))

    return sorted(filenames, key=extract_numbers)

# Function to read data from HDF5 files
def read_hdf5_files(folder_path):
    filenames = [f for f in os.listdir(folder_path) if f.endswith('.h5')]
    sorted_filenames = sort_filenames(filenames)

    for file_name in sorted_filenames:
        file_path = os.path.join(folder_path, file_name)
        with h5py.File(file_path, 'r') as f:
            # Iterate through the keys in the HDF5 file to find the correct dataset name
            for key in f.keys():
                data = f[key][()]  # Try reading the dataset using the identified key
                hdf5_data.append((file_name, data))

# Function to read data from CSV file
def read_csv_file(file_path):
    csv_df = pd.read_csv(file_path)
    csv_values = csv_df.iloc[:, 1]  # Extract values from the second column
    csv_data.extend(csv_values.tolist())

# Call functions to read data
read_hdf5_files(hdf5_folder_path)
read_csv_file(csv_file_path)

# Verify the lengths of both datasets
print("Number of HDF5 files read:", len(hdf5_data))
print("Number of values read from CSV file:", len(csv_data))

# Labeling the HDF5 data with CSV values
labeled_data = []
for i, (file_name, data) in enumerate(hdf5_data):
    if i < len(csv_data):  # Ensure there are enough labels for the HDF5 data
        labeled_data.append((file_name, data, csv_data[i]))

# Print the filenames, shapes, and their corresponding labels
for item in labeled_data:
    print("Filename:", item[0])
    print("Shape:", item[1].shape)
    print("Label:", item[2])

# Optional: Save labeled data to a new file or process as needed

# Preprocessing 

import os
import re
import h5py
import numpy as np
import pandas as pd
from sklearn.decomposition import PCA

def load_dataset(folder_path, csv_file_path, num_components=15):
    # Load the CSV file containing the corresponding values
    csv_data = pd.read_csv(csv_file_path)

    # Extract the second column of the CSV file (assuming index 1 is the second column)
    csv_values = csv_data.iloc[:, 1].values

    # Initialize list to store HDF5 data and corresponding filenames
    hdf5_data = []
    hdf5_filenames = []

    # Define a function to extract numerical parts from filenames for proper sorting
    def natural_key(filename):
        return [int(text) if text.isdigit() else text for text in re.split(r'(\d+)', filename)]

    # Get the list of HDF5 files and sort them using the natural key
    hdf5_files = sorted([file for file in os.listdir(folder_path) if file.endswith('.h5')], key=natural_key)

    # Loop through the sorted HDF5 files
    for file_name in hdf5_files:
        file_path = os.path.join(folder_path, file_name)
        with h5py.File(file_path, 'r') as f:
            # Iterate through keys to find the correct dataset name
            for key in f.keys():
                data = f[key][()]  # Try reading with the found key
                hdf5_data.append(data)
                hdf5_filenames.append(file_name)

    # Stack the individual NumPy arrays into a single array
    if len(hdf5_data) > 0:
        hdf5_data = np.stack(hdf5_data, axis=0)  # Stack the arrays
    else:
        raise ValueError("No HDF5 data found in the specified folder.")

    # Reshape the data for PCA: (num_samples, height, width, num_bands) to (num_samples*height*width, num_bands)
    num_samples, height, width, num_bands = hdf5_data.shape
    reshaped_data = hdf5_data.reshape(-1, num_bands)

    # Apply PCA
    pca = PCA(n_components=num_components)
    pca_data = pca.fit_transform(reshaped_data)

    # Reshape back to original shape but with reduced bands
    pca_data = pca_data.reshape(num_samples, height, width, num_components)

    # Print the significant bands
    components = pca.components_
    significant_bands = np.argsort(-np.abs(components), axis=1)[:, :num_components]
    
    for i, component in enumerate(significant_bands):
        print(f"Principal Component {i + 1}: Significant Bands: {component}")

    # Check the number of samples loaded from HDF5 files
    print("Number of samples loaded from HDF5 files:", len(pca_data))

    # Check the number of samples loaded from the CSV file
    print("Number of samples loaded from CSV file:", len(csv_values))

    # Check data consistency
    num_samples_hdf5 = pca_data.shape[0]
    num_samples_csv = len(csv_values)
    if num_samples_hdf5 != num_samples_csv:
        raise ValueError("Mismatched number of samples between HDF5 files and CSV data.")

    return pca_data, csv_values, hdf5_filenames, csv_data

num_components = 15  # Number of principal components to keep

hdf5_data, csv_values, hdf5_filenames, csv_data = load_dataset(hdf5_folder_path, csv_file_path, num_components)

# Verify the loaded data
print("Shape of HDF5 data array:", hdf5_data.shape)
print("Number of samples loaded from HDF5 files:", len(hdf5_data))
print("Number of samples loaded from CSV file:", len(csv_values))


#Data Augmentation

def augment_and_replicate_data(pca_data, csv_values, pca_filenames, augmentation, n_augmentations=1):
    augmented_pca_data = []
    augmented_csv_values = []
    augmented_filenames = []

    for idx in range(len(pca_data)):
        original_data = pca_data[idx]
        original_csv_value = csv_values[idx]
        original_filename = pca_filenames[idx]

        # Append original data and corresponding CSV value
        augmented_pca_data.append(original_data)
        augmented_csv_values.append(original_csv_value)
        augmented_filenames.append(f"{original_filename}_original")

        # Apply augmentation n_augmentations times to the original data
        for aug_idx in range(n_augmentations):
            augmented_data = augmentation.augment_image(original_data)
            augmented_pca_data.append(augmented_data)
            augmented_csv_values.append(original_csv_value)
            augmented_filenames.append(f"{original_filename}_aug{aug_idx+1}")

    # Convert lists to NumPy arrays
    augmented_pca_data = np.array(augmented_pca_data)
    augmented_csv_values = np.array(augmented_csv_values)

    return augmented_pca_data, augmented_csv_values, augmented_filenames


# Load dataset with PCA
pca_data, csv_values, hdf5_filenames, csv_data = load_dataset(hdf5_folder_path, csv_file_path, num_components)
train_data, test_data, train_labels, test_labels, train_filenames, test_filenames = train_test_split(pca_data, csv_values, hdf5_filenames, test_size=0.2, random_state=42)

# Define augmentation pipeline
augmentation = iaa.Sequential([
    iaa.Affine(translate_px={"x": (-10, 10), "y": (-10, 10)}),  # Translation
    iaa.Affine(rotate=(-25, 25)),  # Rotation
    iaa.Affine(scale=(0.8, 1.2)),  # Scaling
    iaa.Fliplr(0.5)  # Flipping
])

# Number of augmentations per original sample
n_augmentations = 20

# Augment and replicate data
augmented_pca_data, augmented_csv_values, augmented_filenames = augment_and_replicate_data(train_data, train_labels, train_filenames, augmentation, n_augmentations)

# Verify shapes and consistency after augmentation
print("Augmented PCA data shape:", augmented_pca_data.shape)
print("Augmented CSV data shape:", augmented_csv_values.shape)

# Verification of consistency
if augmented_pca_data.shape[0] == len(augmented_csv_values):
    print("Number of samples matches between augmented PCA data and augmented CSV data.")
else:
    print("Mismatched number of samples between augmented PCA data and augmented CSV data.")

# Print first 10 corresponding PCA data and CSV values
for i, (filename, data, csv_value) in enumerate(zip(augmented_filenames, augmented_pca_data, augmented_csv_values)):
    if i >= 10:
        break
    print(f"Filename {i+1}: {filename}")
    print(f"Augmented PCA Data {i+1}: {data.shape}")
    print(f"Augmented CSV Value {i+1}: {csv_value}")
    print("---")

#Creating a CNN Model
import torch
import torch.nn as nn
from torchsummary import summary

# Define SSRN model with Dropout and Batch Normalization
class SSRN(nn.Module):
    def __init__(self, dropout_rate=0.25):
        super(SSRN, self).__init__()
        self.conv1 = nn.Conv2d(in_channels=15, out_channels=32, kernel_size=3, stride=1, padding=1)
        self.bn1 = nn.BatchNorm2d(32)
        self.relu1 = nn.ReLU()
        self.maxpool1 = nn.MaxPool2d(kernel_size=2, stride=2)
        self.dropout1 = nn.Dropout(dropout_rate)

        self.conv2 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, stride=1, padding=1)
        self.bn2 = nn.BatchNorm2d(64)
        self.relu2 = nn.ReLU()
        self.maxpool2 = nn.MaxPool2d(kernel_size=2, stride=2)
        self.dropout2 = nn.Dropout(dropout_rate)

        self.conv3 = nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, stride=1, padding=1)
        self.bn3 = nn.BatchNorm2d(128)
        self.relu3 = nn.ReLU()
        self.maxpool3 = nn.MaxPool2d(kernel_size=2, stride=2)
        self.dropout3 = nn.Dropout(dropout_rate)

        self.conv4 = nn.Conv2d(in_channels=128, out_channels=256, kernel_size=3, stride=1, padding=1)
        self.bn4 = nn.BatchNorm2d(256)
        self.relu4 = nn.ReLU()
        self.maxpool4 = nn.MaxPool2d(kernel_size=2, stride=2)
        self.dropout4 = nn.Dropout(dropout_rate)

        self.flatten = nn.Flatten()

    def forward(self, x):
        x = self.conv1(x)
        x = self.bn1(x)
        x = self.relu1(x)
        x = self.maxpool1(x)
        x = self.dropout1(x)

        x = self.conv2(x)
        x = self.bn2(x)
        x = self.relu2(x)
        x = self.maxpool2(x)
        x = self.dropout2(x)

        x = self.conv3(x)
        x = self.bn3(x)
        x = self.relu3(x)
        x = self.maxpool3(x)
        x = self.dropout3(x)

        x = self.conv4(x)
        x = self.bn4(x)
        x = self.relu4(x)
        x = self.maxpool4(x)
        x = self.dropout4(x)

        x = self.flatten(x)
        return x

# Create SSRN model instance with a custom dropout rate
dropout_rate = 0.35  # You can change this value to adjust the dropout rate
ssrn_model = SSRN(dropout_rate=dropout_rate)

# Move model to GPU if available
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
ssrn_model.to(device)

# Print SSRN model summary
summary(ssrn_model, (15, 40, 40))

# Calculate the output dimension of the flatten layer
sample_input = torch.randn(1, 15, 40, 40).to(device)
flatten_output = ssrn_model(sample_input)
print(f"Flatten layer output dimension: {flatten_output.size(1)}")

# Define ViT model
class VisionTransformer(nn.Module):
    def __init__(self, input_dim=1024, emb_size=128, num_layers=4, num_heads=4, num_classes=1, dropout=0.1):
        super(VisionTransformer, self).__init__()
        self.embedding = nn.Linear(input_dim, emb_size)
        self.cls_token = nn.Parameter(torch.randn(1, 1, emb_size))
        self.pos_embedding = nn.Parameter(torch.randn(1, 1 + input_dim, emb_size))
        self.dropout = nn.Dropout(dropout)
        self.transformer_blocks = nn.ModuleList([
            TransformerBlock(emb_size, num_heads, dropout=dropout) for _ in range(num_layers)
        ])
        self.mlp_head = nn.Sequential(
            nn.LayerNorm(emb_size),
            nn.Linear(emb_size, num_classes)
        )

    def forward(self, x):
        batch_size = x.size(0)
        x = self.embedding(x)
        cls_tokens = self.cls_token.expand(batch_size, -1, -1)
        x = torch.cat((cls_tokens, x.unsqueeze(1)), dim=1)
        x = x + self.pos_embedding[:, :x.size(1), :]
        x = self.dropout(x)

        for block in self.transformer_blocks:
            x = block(x)

        x = x[:, 0]
        x = self.mlp_head(x)
        return x

# Define MultiHeadAttention and TransformerBlock classes
class MultiHeadAttention(nn.Module):
    def __init__(self, emb_size, num_heads):
        super(MultiHeadAttention, self).__init__()
        self.emb_size = emb_size
        self.num_heads = num_heads
        self.qkv = nn.Linear(emb_size, emb_size * 3)
        self.fc = nn.Linear(emb_size, emb_size)

    def forward(self, x):
        batch_size, n_patches, emb_size = x.size()
        qkv = self.qkv(x)
        qkv = qkv.view(batch_size, n_patches, self.num_heads, 3 * emb_size // self.num_heads)
        qkv = qkv.permute(0, 2, 1, 3).contiguous()
        q, k, v = qkv.chunk(3, dim=-1)

        scores = q @ k.transpose(-2, -1) / (emb_size ** 0.5)
        attn = torch.softmax(scores, dim=-1)
        context = attn @ v
        context = context.permute(0, 2, 1, 3).contiguous()
        context = context.view(batch_size, n_patches, emb_size)

        out = self.fc(context)
        return out

class TransformerBlock(nn.Module):
    def __init__(self, emb_size, num_heads, ff_hidden_mult=4, dropout=0.1):
        super(TransformerBlock, self).__init__()
        self.norm1 = nn.LayerNorm(emb_size)
        self.norm2 = nn.LayerNorm(emb_size)
        self.mha = MultiHeadAttention(emb_size, num_heads)
        self.ff = nn.Sequential(
            nn.Linear(emb_size, ff_hidden_mult * emb_size),
            nn.ReLU(),
            nn.Linear(ff_hidden_mult * emb_size, emb_size),
        )
        self.dropout = nn.Dropout(dropout)

    def forward(self, x):
        x = x + self.dropout(self.mha(self.norm1(x)))
        x = x + self.dropout(self.ff(self.norm2(x)))
        return x

# Create an instance of VisionTransformer
vit_model = VisionTransformer()

# Move model to GPU if available
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
vit_model.to(device)

# Print ViT model summary
summary(vit_model, (1024,))

# Define Combined Model
class CombinedModel(nn.Module):
    def __init__(self):
        super(CombinedModel, self).__init__()
        self.ssrn = SSRN()  # Define your SSRN model separately
        self.vit = VisionTransformer()  # Update based on SSRN output

    def forward(self, x):
        x = self.ssrn(x)  # Pass through SSRN
        x = x.view(x.size(0), -1)  # Flatten the output for ViT
        x = self.vit(x)  # Pass through ViT
        return x

# Create an instance of CombinedModel
combined_model = CombinedModel()

# Move model to GPU if available
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
combined_model.to(device)

# Print model summary
summary(combined_model, (15, 40, 40))

#Training

import threading
import sys
import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, TensorDataset
from sklearn.model_selection import train_test_split
from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error
import matplotlib.pyplot as plt
import random

# Define metrics
def calculate_r2(targets, predictions):
    return r2_score(targets, predictions)

def calculate_rmse(targets, predictions):
    return np.sqrt(mean_squared_error(targets, predictions))

def calculate_rpd(targets, predictions):
    return np.std(targets) / calculate_rmse(targets, predictions)

# Function to listen for manual stop input
def listen_for_stop():
    global manual_stop
    input("Press Enter to stop training...\n")
    manual_stop = True

# Function for testing the model
def test_model(test_loader, model, device):
    model.eval()
    test_predictions = []
    test_targets = []
    with torch.no_grad():
        for data, target in test_loader:
            data, target = data.to(device), target.to(device)
            output = model(data)
            test_predictions.extend(output.cpu().numpy().flatten())
            test_targets.extend(target.cpu().numpy().flatten())
    
    test_r2 = calculate_r2(np.array(test_targets), np.array(test_predictions))
    test_rmse = calculate_rmse(np.array(test_targets), np.array(test_predictions))
    test_rpd = calculate_rpd(np.array(test_targets), np.array(test_predictions))
    print(f"Test RÂ²: {test_r2:.2f}, Test RMSE: {test_rmse:.4f}, Test RPD: {test_rpd:.4f}")
    
    return test_r2, test_rmse, test_rpd

# Start the manual stop listener in a separate thread
manual_stop = False
stop_thread = threading.Thread(target=listen_for_stop)
stop_thread.start()

try:
    # Example data loading and augmentation (replace with actual functions)
    augmented_pca_data, augmented_csv_values, augmented_filenames = augment_and_replicate_data(train_data, train_labels, train_filenames, augmentation, n_augmentations)

    # Prepare data
    X = augmented_pca_data
    y = augmented_csv_values.reshape(-1, 1)
    
    # Split the data into train (60%), val (20%), and test (20%)
    X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.4, random_state=42)
    X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)

    # Convert to PyTorch tensors
    X_train_tensor = torch.tensor(X_train, dtype=torch.float32).permute(0, 3, 1, 2)
    X_val_tensor = torch.tensor(X_val, dtype=torch.float32).permute(0, 3, 1, 2)
    X_test_tensor = torch.tensor(X_test, dtype=torch.float32).permute(0, 3, 1, 2)
    
    y_train_tensor = torch.tensor(y_train, dtype=torch.float32)
    y_val_tensor = torch.tensor(y_val, dtype=torch.float32)
    y_test_tensor = torch.tensor(y_test, dtype=torch.float32)

    # Create datasets and data loaders
    train_dataset = TensorDataset(X_train_tensor, y_train_tensor)
    val_dataset = TensorDataset(X_val_tensor, y_val_tensor)
    test_dataset = TensorDataset(X_test_tensor, y_test_tensor)

    train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)
    val_loader = DataLoader(val_dataset, batch_size=64, shuffle=False)
    test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)

    # Model and optimization setup
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    combined_model = CombinedModel().to(device)
    optimizer = optim.AdamW(combined_model.parameters(), lr=0.01, weight_decay=0.1)
    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', factor=0.65, patience=20, verbose=True)
    criterion = nn.HuberLoss(delta=1)

    patience = 40
    best_val_loss = float('inf')
    epochs_no_improve = 0
    early_stop = False
    num_epochs = 1000

    # Initialize lists to store the metrics for plotting
    train_loss_list = []
    val_loss_list = []
    train_r2s = []
    val_r2s = []
    train_rmses = []
    val_rmses = []
    train_rpds = []
    val_rpds = []

    # Start training loop
    for epoch in range(num_epochs):
        if early_stop or manual_stop:
            print(f"Early stopping at epoch {epoch}")
            break
        
        combined_model.train()
        running_loss = 0.0
        epoch_train_targets = []
        epoch_train_predictions = []

        for data, target in train_loader:
            data, target = data.to(device), target.to(device)
            optimizer.zero_grad()
            output = combined_model(data)
            loss = criterion(output, target)
            loss.backward()
            nn.utils.clip_grad_norm_(combined_model.parameters(), max_norm=1.0)
            optimizer.step()

            running_loss += loss.item()
            epoch_train_predictions.extend(output.detach().cpu().numpy().flatten())
            epoch_train_targets.extend(target.detach().cpu().numpy().flatten())

        train_loss = running_loss / len(train_loader)
        train_r2 = calculate_r2(np.array(epoch_train_targets), np.array(epoch_train_predictions))
        train_rmse = calculate_rmse(np.array(epoch_train_targets), np.array(epoch_train_predictions))
        train_rpd = calculate_rpd(np.array(epoch_train_targets), np.array(epoch_train_predictions))

        combined_model.eval()
        val_loss = 0.0
        epoch_val_targets = []
        epoch_val_predictions = []

        with torch.no_grad():
            for data, target in val_loader:
                data, target = data.to(device), target.to(device)
                output = combined_model(data)
                val_loss += criterion(output, target).item()
                epoch_val_predictions.extend(output.detach().cpu().numpy().flatten())
                epoch_val_targets.extend(target.detach().cpu().numpy().flatten())

        val_loss /= len(val_loader)
        val_r2 = calculate_r2(np.array(epoch_val_targets), np.array(epoch_val_predictions))
        val_rmse = calculate_rmse(np.array(epoch_val_targets), np.array(epoch_val_predictions))
        val_rpd = calculate_rpd(np.array(epoch_val_targets), np.array(epoch_val_predictions))
        scheduler.step(val_loss)

        print(f"Epoch {epoch + 1}/{num_epochs}, Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}, Train RÂ²: {train_r2:.2f}, Val RÂ²: {val_r2:.2f}, Train RMSE: {train_rmse:.4f}, Val RMSE: {val_rmse:.4f}, Train RPD: {train_rpd:.4f}, Val RPD: {val_rpd:.4f}")

        # Store metrics for plotting
        train_loss_list.append(train_loss)
        val_loss_list.append(val_loss)
        train_r2s.append(train_r2)
        val_r2s.append(val_r2)
        train_rmses.append(train_rmse)
        val_rmses.append(val_rmse)
        train_rpds.append(train_rpd)
        val_rpds.append(val_rpd)

        if val_loss < best_val_loss:
            best_val_loss = val_loss
            epochs_no_improve = 0
            torch.save(combined_model.state_dict(), 'Project/models/Testing_SSRN_with_PCA-15.pth')
        else:
            epochs_no_improve += 1
            if epochs_no_improve >= patience:
                early_stop = True

        # Test the model at the end of every epoch
        print(f"\nTesting after epoch {epoch + 1}:")
        test_r2, test_rmse, test_rpd = test_model(test_loader, combined_model, device)

    # Load and test the best model after training
    combined_model.load_state_dict(torch.load('Project/models/Testing_SSRN_with_PCA-15.pth'))
    print("\nFinal Test Metrics with Best Model:")
    test_r2, test_rmse, test_rpd = test_model(test_loader, combined_model, device)

finally:
    manual_stop = True
    stop_thread.join()

#validating
# Load the best saved model
combined_model = CombinedModel()
combined_model.load_state_dict(torch.load('Project/models/Testing_SSRN_with_PCA-15.pth', weights_only=True))
combined_model.eval()
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
combined_model.to(device)

# Plotting training and validation metrics
plt.figure(figsize=(12, 10))

plt.subplot(2, 2, 1)
plt.plot(range(1, len(train_loss_list) + 1), train_loss_list, label='Training Loss')
plt.plot(range(1, len(val_loss_list) + 1), val_loss_list, label='Validation Loss')
plt.title('Training and Validation Loss over Epochs')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.legend()
plt.grid(True)

plt.subplot(2, 2, 2)
plt.plot(range(1, len(train_r2s) + 1), train_r2s, label='Training RÂ²')
plt.plot(range(1, len(val_r2s) + 1), val_r2s, label='Validation RÂ²')
plt.title('Training and Validation RÂ² over Epochs')
plt.xlabel('Epoch')
plt.ylabel('RÂ² Score')
plt.legend()
plt.grid(True)

plt.subplot(2, 2, 3)
plt.plot(range(1, len(train_rmses) + 1), train_rmses, label='Training RMSE')
plt.plot(range(1, len(val_rmses) + 1), val_rmses, label='Validation RMSE')
plt.title('Training and Validation RMSE over Epochs')
plt.xlabel('Epoch')
plt.ylabel('RMSE')
plt.legend()
plt.grid(True)

plt.subplot(2, 2, 4)
plt.plot(range(1, len(train_rpds) + 1), train_rpds, label='Training RPD')
plt.plot(range(1, len(val_rpds) + 1), val_rpds, label='Validation RPD')
plt.title('Training and Validation RPD over Epochs')
plt.xlabel('Epoch')
plt.ylabel('RPD')
plt.legend()
plt.grid(True)

plt.tight_layout()
plt.savefig("Training_Validation_Metrics.png")
plt.show()